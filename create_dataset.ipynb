{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import tarfile\n",
    "from wikimapper import WikiMapper\n",
    "from datapackage import Package\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CMU Movie Dataset\n",
    "Primary dataset for our analysis, downloaded from the [CMU Official Website](http://www.cs.cmu.edu/~ark/personas/). The main file movie.metadata.tsv is stored in the `data` folder. \n",
    "\n",
    "Before running this, please remove the `data` folder from your working directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MOVIE_CMU_URL = \"http://www.cs.cmu.edu/~ark/personas/data/MovieSummaries.tar.gz\"\n",
    "response = requests.get(MOVIE_CMU_URL, stream=True)\n",
    "file = tarfile.open(fileobj=response.raw, mode=\"r|gz\")\n",
    "file.extractall(path='.')\n",
    "\n",
    "Path(\"MovieSummaries\").rename(\"data\")\n",
    "data_path = Path(\"data\")\n",
    "\n",
    "for file_name in [\"character.metadata.tsv\", \"name.clusters.txt\", \"plot_summaries.txt\", \"README.txt\", \"tvtropes.clusters.txt\"]:\n",
    "    file_path = data_path / file_name\n",
    "    if file_path.exists():\n",
    "        file_path.unlink()\n",
    "\n",
    "cmu_cols = [\"movie_wikipedia_id\", \"movie_freebase_id\", \"movie_title\", \"movie_release\", \"movie_revenue\", \"movie_runtime\", \"movie_languages\", \"movie_countries\", \"movie_genres\"]\n",
    "cmu_df = (pd.read_csv(\n",
    "    data_path / \"movie.metadata.tsv\", \n",
    "    sep=\"\\t\", \n",
    "    header=None, \n",
    "    names=cmu_cols, \n",
    "    usecols=[\"movie_wikipedia_id\", \"movie_title\", \"movie_release\", \"movie_revenue\", \"movie_runtime\",  \"movie_languages\", \"movie_countries\", \"movie_genres\"])\n",
    "    .assign(\n",
    "        movie_release=lambda df: df.movie_release.astype(str).str.slice(0, 4).replace(\"nan\", pd.NA).astype(\"Int32\"),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping Wikipedia IDs to Wikidata IDs\n",
    "\n",
    "Our analysis requires connecting movies to books they're based on. We use the [wikimapper](https://github.com/jcklie/wikimapper) Python library for mapping Wikipedia IDs (available in the CMU movie dataset) to Wikidata IDs (which we will need to join the results of the Wikidata Query Service), requiring a Wikipedia SQL dump for creating an index.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following commands may take around one hour to finish. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-15 15:06:03,101 - wikimapper.download - INFO - Downloading [https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-page.sql.gz] to [data/enwiki-latest-page.sql.gz]\n",
      "2023-11-15 15:13:47,592 - wikimapper.download - INFO - Downloading [https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-page_props.sql.gz] to [data/enwiki-latest-page_props.sql.gz]\n",
      "2023-11-15 15:15:15,525 - wikimapper.download - INFO - Downloading [https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-redirect.sql.gz] to [data/enwiki-latest-redirect.sql.gz]\n",
      "2023-11-15 15:15:52,647 - wikimapper.processor - INFO - Creating index for [enwiki-latest] in [data/index_enwiki-latest.db]\n",
      "2023-11-15 15:15:52,662 - wikimapper.processor - INFO - Parsing pages dump\n",
      "2023-11-15 15:24:09,978 - wikimapper.processor - INFO - Creating database index on 'wikipedia_title'\n",
      "2023-11-15 15:25:20,468 - wikimapper.processor - INFO - Parsing page properties dump\n",
      "2023-11-15 15:28:12,513 - wikimapper.processor - INFO - Parsing redirects dump\n",
      "2023-11-15 16:26:08,033 - wikimapper.processor - INFO - Creating database index on 'wikidata_id'\n"
     ]
    }
   ],
   "source": [
    "!wikimapper download enwiki-latest --dir data\n",
    "!wikimapper create enwiki-latest --dumpdir data --target data/index_enwiki-latest.db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enrich the CMU Movie Dataframe by mapping the wikipedia id to the wikidata id. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = WikiMapper(data_path / \"index_enwiki-latest.db\")\n",
    "cmu_df = (cmu_df.assign(\n",
    "            movie_wikidata_id = lambda x: x.movie_wikipedia_id.apply(\n",
    "                lambda wikipedia_id: mapper.wikipedia_id_to_id(wikipedia_id)\n",
    "                )\n",
    "            )\n",
    "            .drop(columns=[\"movie_wikipedia_id\"])\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying Movies Based on Books Using Wikidata\n",
    "We identify book-adaptations using the Wikidata database, focusing on the P144 (based on) relation. A SPARQL query on Wikidata Query Service helps us extract interconnected pairs of movie and book entities along with book attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/73/4f40pvqd15d7bgql2yrmyvzm0000gn/T/ipykernel_18097/418483006.py:59: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  book_release = lambda x: pd.to_datetime(x.pubDateLabel, errors='coerce').dt.year.astype('Int64')\n"
     ]
    }
   ],
   "source": [
    "WIKI_DATA_SERVICE_URL = 'https://query.wikidata.org/sparql'\n",
    "query = '''\n",
    "SELECT DISTINCT ?movie ?book ?bookLabel ?authorLabel ?instanceOfLabel ?countryLabel ?pubDateLabel ?genreLabel ?awardLabel ?seriesLabel ?goodreadsLabel\n",
    "WHERE \n",
    "{\n",
    "  VALUES ?bookType { wd:Q47461344 wd:Q7725634 wd:Q571 wd:Q14406742 wd:Q21198342 wd:Q277759 }\n",
    "  VALUES ?movieType { wd:Q11424 wd:Q506240 }\n",
    "\n",
    "  ?book wdt:P31 ?bookType.\n",
    "  OPTIONAL {?book wdt:P50 ?author}\n",
    "  OPTIONAL {?book wdt:P31 ?instanceOf}\n",
    "  OPTIONAL {?book wdt:P495 ?country}\n",
    "  OPTIONAL {?book wdt:P577 ?pubDate}\n",
    "  OPTIONAL {?book wdt:P136 ?genre}\n",
    "  OPTIONAL {?book wdt:P166 ?award}\n",
    "  OPTIONAL {?book wdt:P179 ?series}\n",
    "  OPTIONAL {?book wdt:P8383 ?goodreads}\n",
    "\n",
    "  ?movie wdt:P31 ?movieType;          \n",
    "         wdt:P144 ?book.\n",
    "\n",
    "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "}\n",
    "'''\n",
    "query_result = requests.get(WIKI_DATA_SERVICE_URL, params = {'format': 'json', 'query': query})\n",
    "wikidata_df =pd.DataFrame(query_result.json()['results']['bindings'])\n",
    "for column in wikidata_df.columns:\n",
    "    wikidata_df[column] = wikidata_df[column].apply(lambda x: x['value'] if isinstance(x, dict) and 'value' in x else x)\n",
    "  \n",
    "def get_list(series: pd.Series) -> list:\n",
    "    return list(set(series.dropna().tolist()))\n",
    "\n",
    "def mode(x: pd.Series) -> pd.Series:\n",
    "    modes = x.mode()\n",
    "    if len(modes) > 0:\n",
    "        return modes.iloc[0]\n",
    "    return None\n",
    "\n",
    "categories = {\n",
    "    'fiction': {'novel', 'short novel', 'novella', 'serialized fiction', 'short story', 'war fiction', 'magic realist fiction', 'metafiction', 'science fiction', 'suspense in literature', 'horror novel', 'horror fiction', 'crime fiction', 'psychological thriller', 'speculative/fantastic fiction', 'adventure fiction', 'detective fiction', 'noir fiction', 'political novel', 'vampire fiction', 'dystopian fiction', 'social science fiction', 'techno-thriller', 'thriller', 'fantasy', 'Gothic novel', 'picaresque novel', 'mystery fiction', 'post-apocalyptic fiction', 'philosophical fiction', 'romantic fiction', 'Bildungsroman', 'roman Ã  clef', 'comedy', 'black comedy'},\n",
    "    'non_fiction': {'nonfiction', 'memoir', 'autobiography', 'biographical novel', 'biography', 'essay'},\n",
    "    'children': {'children\\'s literature', 'children\\'s fiction', 'young adult fiction', 'children\\'s novel'},\n",
    "    'historical': {'historical fiction', 'historical novel'},\n",
    "    'drama': {'play', 'drama', 'tragedy'},\n",
    "    'anime': {'adventure anime and manga', 'drama anime and manga'},\n",
    "    'fantasy': {'magic realist fiction', 'fantasy', 'vampire fiction', 'fairy tale'},\n",
    "    'science_fiction': {'science fiction', 'dystopian fiction', 'social science fiction', 'techno-thriller', 'post-apocalyptic fiction'},\n",
    "    'horror': {'horror novel', 'horror fiction'},\n",
    "    'thriller': {'psychological thriller', 'thriller'},\n",
    "    'detective': {'detective fiction', 'noir fiction', 'mystery fiction', 'cloak and dagger novel'},\n",
    "    'satire': {'satire', 'satirical fiction', 'metafiction'},\n",
    "    'comedy': {'comedy', 'black comedy'},\n",
    "}\n",
    "\n",
    "wikidata_df = (wikidata_df\n",
    "                .assign(\n",
    "                    movie_wikidata_id = lambda x: x.movie.str.split('/').str[-1],\n",
    "                    book_wikidata_id = lambda x: x.book.str.split('/').str[-1],\n",
    "                    book_release = lambda x: pd.to_datetime(x.pubDateLabel, errors='coerce').dt.year.astype('Int64')\n",
    "                )\n",
    "                .groupby(['movie_wikidata_id', 'book_wikidata_id'])\n",
    "                .agg(\n",
    "                    book_title = pd.NamedAgg(column='bookLabel', aggfunc=mode),\n",
    "                    book_author = ('authorLabel', 'first'),\n",
    "                    book_release = ('book_release', 'first'),\n",
    "                    book_country = ('countryLabel', 'first'),\n",
    "                    book_goodreads_id = ('goodreadsLabel', 'first'),\n",
    "                    series = ('seriesLabel', 'first'),\n",
    "                    instance_of = pd.NamedAgg(column='instanceOfLabel', aggfunc=get_list),\n",
    "                    genre = pd.NamedAgg(column='genreLabel', aggfunc=get_list),\n",
    "                    award = pd.NamedAgg(column='awardLabel', aggfunc=get_list)\n",
    "                )\n",
    "                .assign(\n",
    "                    book_part_of_series = lambda x: x.series.notnull().astype(int),\n",
    "                    literary_work = lambda x: x.instance_of.apply(lambda y: 'literary work' in y).astype(int),\n",
    "                    written_work = lambda x: x.instance_of.apply(lambda y: 'written work' in y).astype(int),\n",
    "                    comic_book_seris = lambda x: x.instance_of.apply(lambda y: 'comic book series' in y).astype(int),\n",
    "                    book_series = lambda x: x.instance_of.apply(lambda y: 'book series' in y).astype(int),\n",
    "                    manga_series = lambda x: x.instance_of.apply(lambda y: 'manga series' in y).astype(int),\n",
    "                    book_fiction = lambda x: x.genre.apply(lambda y: len(set(y).intersection(categories['fiction'])) > 0).astype(int),\n",
    "                    book_non_fiction = lambda x: x.genre.apply(lambda y: len(set(y).intersection(categories['non_fiction'])) > 0).astype(int),\n",
    "                    book_children = lambda x: x.genre.apply(lambda y: len(set(y).intersection(categories['children'])) > 0).astype(int),\n",
    "                    book_historical = lambda x: x.genre.apply(lambda y: len(set(y).intersection(categories['historical'])) > 0).astype(int),\n",
    "                    book_drama = lambda x: x.genre.apply(lambda y: len(set(y).intersection(categories['drama'])) > 0).astype(int),\n",
    "                    book_anime = lambda x: x.genre.apply(lambda y: len(set(y).intersection(categories['anime'])) > 0).astype(int),\n",
    "                    book_fantasy = lambda x: x.genre.apply(lambda y: len(set(y).intersection(categories['fantasy'])) > 0).astype(int),\n",
    "                    book_science_fiction = lambda x: x.genre.apply(lambda y: len(set(y).intersection(categories['science_fiction'])) > 0).astype(int),\n",
    "                    book_horror = lambda x: x.genre.apply(lambda y: len(set(y).intersection(categories['horror'])) > 0).astype(int),\n",
    "                    book_thriller = lambda x: x.genre.apply(lambda y: len(set(y).intersection(categories['thriller'])) > 0).astype(int),\n",
    "                    book_detective = lambda x: x.genre.apply(lambda y: len(set(y).intersection(categories['detective'])) > 0).astype(int),\n",
    "                    book_satire = lambda x: x.genre.apply(lambda y: len(set(y).intersection(categories['satire'])) > 0).astype(int),\n",
    "                    book_comedy = lambda x: x.genre.apply(lambda y: len(set(y).intersection(categories['comedy'])) > 0).astype(int),\n",
    "                    book_won_price = lambda x: x.award.apply(lambda y: len(y) > 0).astype(int),\n",
    "                )\n",
    "                .drop(['instance_of', 'genre', 'award', 'series'], axis=1)\n",
    "                .reset_index()\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Book Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WIKI_DATA_SERVICE_URL = 'https://query.wikidata.org/sparql'\n",
    "query = '''\n",
    "SELECT DISTINCT ?book\n",
    "WHERE \n",
    "{\n",
    "  VALUES ?bookType { wd:Q47461344 wd:Q7725634 wd:Q571 wd:Q14406742 wd:Q21198342 wd:Q277759 }\n",
    "  ?book wdt:P31 ?bookType.\n",
    "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "}\n",
    "'''\n",
    "query_result = requests.get(WIKI_DATA_SERVICE_URL, params = {'format': 'json', 'query': query})\n",
    "book_df = pd.DataFrame(query_result.json()['results']['bindings'])\n",
    "for column in book_df.columns:\n",
    "    book_df[column] = book_df[column].apply(lambda x: x['value'] if isinstance(x, dict) and 'value' in x else x)\n",
    "book_ids = book_df.book.str.split('/').str[-1].tolist()\n",
    "\n",
    "def get_book_wikidata(ids: list):\n",
    "    book_ids = \" \".join([f\"wd:{id}\" for id in ids])\n",
    "    query = '''SELECT DISTINCT ?movie ?book ?bookLabel ?authorLabel ?instanceOfLabel ?countryLabel ?pubDateLabel ?genreLabel ?awardLabel ?seriesLabel\n",
    "                WHERE \n",
    "                {\n",
    "                  VALUES ?book {{ {book_ids} }}\n",
    "                  OPTIONAL {?book wdt:P50 ?author}\n",
    "                  OPTIONAL {?book wdt:P31 ?instanceOf}\n",
    "                  OPTIONAL {?book wdt:P495 ?country}\n",
    "                  OPTIONAL {?book wdt:P577 ?pubDate}\n",
    "                  OPTIONAL {?book wdt:P136 ?genre}\n",
    "                  OPTIONAL {?book wdt:P166 ?award}\n",
    "                  OPTIONAL {?book wdt:P179 ?series}\n",
    "\n",
    "                  SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "                }\n",
    "            '''\n",
    "    query_result = requests.get(WIKI_DATA_SERVICE_URL, params = {'format': 'json', 'query': query.format(book_ids=book_ids)})\n",
    "    df = pd.DataFrame(query_result.json()['results']['bindings'])\n",
    "    for column in df.columns:\n",
    "        df[column] = df[column].apply(lambda x: x['value'] if isinstance(x, dict) and 'value' in x else x)\n",
    "    return df\n",
    "\n",
    "book_df_list = []\n",
    "for i in range(0, len(book_ids), 100):\n",
    "    book_df_list.append(get_book_wikidata(book_ids[i:i+100]))\n",
    "book_df = pd.concat(book_df_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_df = (book_df\n",
    "                .assign(\n",
    "                    book_wikidata_id = lambda x: x.book.str.split('/').str[-1],\n",
    "                    book_release = lambda x: pd.to_datetime(x.pubDateLabel, errors='coerce').dt.year.astype('Int64')\n",
    "                )\n",
    "                .groupby(['book_wikidata_id'])\n",
    "                .agg(\n",
    "                    book_title = pd.NamedAgg(column='bookLabel', aggfunc=mode),\n",
    "                    book_author = ('authorLabel', 'first'),\n",
    "                    book_release = ('book_release', 'first'),\n",
    "                    book_country = ('countryLabel', 'first'),\n",
    "                    series = ('seriesLabel', 'first'),\n",
    "                    instance_of = pd.NamedAgg(column='instanceOfLabel', aggfunc=get_list),\n",
    "                    genre = pd.NamedAgg(column='genreLabel', aggfunc=get_list),\n",
    "                    award = pd.NamedAgg(column='awardLabel', aggfunc=get_list)\n",
    "                )\n",
    "                .assign(\n",
    "                    book_part_of_series = lambda x: x.series.notnull().astype(int),\n",
    "                    literary_work = lambda x: x.instance_of.apply(lambda y: 'literary work' in y).astype(int),\n",
    "                    written_work = lambda x: x.instance_of.apply(lambda y: 'written work' in y).astype(int),\n",
    "                    comic_book_seris = lambda x: x.instance_of.apply(lambda y: 'comic book series' in y).astype(int),\n",
    "                    book_series = lambda x: x.instance_of.apply(lambda y: 'book series' in y).astype(int),\n",
    "                    manga_series = lambda x: x.instance_of.apply(lambda y: 'manga series' in y).astype(int),\n",
    "                    book_fiction = lambda x: x.genre.apply(lambda y: len(set(y).intersection(categories['fiction'])) > 0).astype(int),\n",
    "                    book_non_fiction = lambda x: x.genre.apply(lambda y: len(set(y).intersection(categories['non_fiction'])) > 0).astype(int),\n",
    "                    book_children = lambda x: x.genre.apply(lambda y: len(set(y).intersection(categories['children'])) > 0).astype(int),\n",
    "                    book_historical = lambda x: x.genre.apply(lambda y: len(set(y).intersection(categories['historical'])) > 0).astype(int),\n",
    "                    book_drama = lambda x: x.genre.apply(lambda y: len(set(y).intersection(categories['drama'])) > 0).astype(int),\n",
    "                    book_anime = lambda x: x.genre.apply(lambda y: len(set(y).intersection(categories['anime'])) > 0).astype(int),\n",
    "                    book_fantasy = lambda x: x.genre.apply(lambda y: len(set(y).intersection(categories['fantasy'])) > 0).astype(int),\n",
    "                    book_science_fiction = lambda x: x.genre.apply(lambda y: len(set(y).intersection(categories['science_fiction'])) > 0).astype(int),\n",
    "                    book_horror = lambda x: x.genre.apply(lambda y: len(set(y).intersection(categories['horror'])) > 0).astype(int),\n",
    "                    book_thriller = lambda x: x.genre.apply(lambda y: len(set(y).intersection(categories['thriller'])) > 0).astype(int),\n",
    "                    book_detective = lambda x: x.genre.apply(lambda y: len(set(y).intersection(categories['detective'])) > 0).astype(int),\n",
    "                    book_satire = lambda x: x.genre.apply(lambda y: len(set(y).intersection(categories['satire'])) > 0).astype(int),\n",
    "                    book_comedy = lambda x: x.genre.apply(lambda y: len(set(y).intersection(categories['comedy'])) > 0).astype(int),\n",
    "                    book_won_price = lambda x: x.award.apply(lambda y: len(y) > 0).astype(int),\n",
    "                )\n",
    "                .drop(['instance_of', 'genre', 'award', 'series'], axis=1)\n",
    "                .reset_index()\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Kaggle API for Dataset Access\n",
    "\n",
    "Some of the datasets we use are from Kaggle. To access certain datasets on Kaggle, you'll need a Kaggle account and an API token. Kaggle provides an API that allows you to programmatically download datasets directly into your Jupyter Notebook environment. Here's how to set up and use the Kaggle API token:\n",
    "\n",
    "### 1. Kaggle Account and Token\n",
    "\n",
    "If you don't already have a Kaggle account, you can sign up for one at [Kaggle](https://www.kaggle.com/). Once you have an account, follow these steps to create an API token:\n",
    "\n",
    "- Log in to your Kaggle account.\n",
    "- Go to your account settings page by clicking on your profile picture in the upper right-hand corner of the Kaggle website and selecting \"Account.\"\n",
    "- Scroll down to the \"API\" section and click on the \"Create New API Token\" button. This will download a file called `kaggle.json` containing your API credentials.\n",
    "\n",
    "### 2. Storing the Kaggle API Token\n",
    "\n",
    "To use the Kaggle API in your Jupyter Notebook, you need to store the `kaggle.json` at a designated folder. To do copy the `kaggle.json` in your working directory and run the following commands:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: /Users/nifleisch/.kaggle: File exists\n",
      "mv: kaggle.json: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "! mkdir ~/.kaggle\n",
    "! mv kaggle.json ~/.kaggle/\n",
    "! chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goodreads Dataset\n",
    "For further information about the books, we use a [Kaggle](https://www.kaggle.com) dataset curated from [Goodreads](https://www.goodreads.com). In particular we will investigate how the book ratings on goodread influence the respective book-adaptations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading goodreads-book-datasets-10m.zip to /Users/nifleisch/dev/uni/ada-2023\n",
      "  7%|âââ                                    | 31.0M/460M [00:03<00:36, 12.5MB/s]^C\n",
      "  7%|âââ                                    | 32.0M/460M [00:03<00:44, 10.1MB/s]\n",
      "User cancelled operation\n",
      "Archive:  goodreads-book-datasets-10m.zip\n",
      "  End-of-central-directory signature not found.  Either this file is not\n",
      "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
      "  latter case the central directory and zipfile comment will be found on\n",
      "  the last disk(s) of this archive.\n",
      "unzip:  cannot find zipfile directory in one of goodreads-book-datasets-10m.zip or\n",
      "        goodreads-book-datasets-10m.zip.zip, and cannot find goodreads-book-datasets-10m.zip.ZIP, period.\n",
      "zsh:1: no matches found: user_rating_*.csv\n",
      "mkdir: data/goodreads: File exists\n",
      "zsh:1: no matches found: data/book*.csv\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets download -d bahramjannesarr/goodreads-book-datasets-10m\n",
    "!unzip goodreads-book-datasets-10m.zip\n",
    "!rm goodreads-book-datasets-10m.zip\n",
    "!rm user_rating_*.csv\n",
    "!mkdir data/goodreads\n",
    "!mv data/book*.csv data/goodreads/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_title(title_series: pd.Series) -> pd.Series:\n",
    "    return (title_series\n",
    "            .str.split('(').str[0]\n",
    "            .str.split(':').str[0]\n",
    "            .str.lower()\n",
    "            .str.replace('and', '&')\n",
    "            .str.replace('.', '')\n",
    "            .str.replace(\"'\", '')\n",
    "            .str.replace('-', ' ')\n",
    "            .str.replace(r'\\s+', ' ', regex=True)\n",
    "            .str.strip()\n",
    "    )\n",
    "\n",
    "def clean_author(author_series: pd.Series) -> pd.Series:\n",
    "    initial_letter = (author_series\n",
    "                      .str.strip()\n",
    "                      .str[0]\n",
    "                      .str.lower())\n",
    "    last_name = (author_series\n",
    "                 .str.split(r\"(\\s|-|')\", regex=True)\n",
    "                 .str[-1]\n",
    "                 .str.replace('.', '')\n",
    "                 .str.replace(\"'\", '')\n",
    "                 .str.replace(r'\\s+', ' ', regex=True)\n",
    "                 .str.strip()\n",
    "                 .str.lower()\n",
    "                 )\n",
    "    return initial_letter + \" \" + last_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikidata_df = wikidata_df.assign(\n",
    "                                join_title = lambda x: clean_title(x.book_title),\n",
    "                                join_author = lambda x: clean_author(x.book_author), \n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_df = book_df.assign(  \n",
    "                            join_title = lambda x: clean_title(x.book_title),\n",
    "                            join_author = lambda x: clean_author(x.book_author), \n",
    "                        )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "for file in data_path.glob('goodreads/book*.csv'):\n",
    "    try:\n",
    "        df = (pd.read_csv(\n",
    "                file, \n",
    "                usecols=['Name', 'Authors', 'Publisher', 'pagesNumber', 'Rating', 'RatingDistTotal']\n",
    "                )\n",
    "                .rename(columns={'pagesNumber': 'book_pages', 'RatingDistTotal': 'book_ratings_count', 'Rating': 'book_rating', \n",
    "                                 'Publisher': 'book_publisher', 'Authors': 'book_author', \n",
    "                                 'Name': 'book_title'})\n",
    "            )\n",
    "        df_list.append(df)\n",
    "    except:\n",
    "        df = (pd.read_csv(\n",
    "                file, \n",
    "                usecols=['Name', 'Authors', 'Publisher', 'PagesNumber', 'Rating', 'RatingDistTotal']\n",
    "                )\n",
    "                .rename(columns={'PagesNumber': 'book_pages', 'RatingDistTotal': 'book_ratings_count', 'Rating': 'book_rating', \n",
    "                                 'Publisher': 'book_publisher', 'Authors': 'book_author', \n",
    "                                 'Name': 'book_title'})\n",
    "            )\n",
    "        df_list.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goodreads_df = (pd.concat(df_list, ignore_index=True)\n",
    "                .assign(\n",
    "                    book_rating = lambda x: x.book_rating.replace(0, np.nan),\n",
    "                    book_pages = lambda x: x.book_pages.replace(0, np.nan).astype('Int64'),\n",
    "                    book_ratings_count = lambda x: x.book_ratings_count.str.split(':').str[-1].astype('Int64'),\n",
    "                    join_title = lambda x: clean_title(x.book_title),\n",
    "                    join_author = lambda x: clean_author(x.book_author), \n",
    "                )\n",
    "                .merge(\n",
    "                    wikidata_df.loc[:, ['join_title', 'join_author']].drop_duplicates(), \n",
    "                    on=['join_title', 'join_author'], \n",
    "                    how='inner'\n",
    "                    )\n",
    "                .drop_duplicates(subset=['join_title', 'join_author'])\n",
    "                .drop(['book_title', 'book_author'], axis=1)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "goodreads_book_df = (pd.concat(df_list, ignore_index=True)\n",
    "                .assign(\n",
    "                    book_rating = lambda x: x.book_rating.replace(0, np.nan),\n",
    "                    book_pages = lambda x: x.book_pages.replace(0, np.nan).astype('Int64'),\n",
    "                    book_ratings_count = lambda x: x.book_ratings_count.str.split(':').str[-1].astype('Int64'),\n",
    "                    join_title = lambda x: clean_title(x.book_title),\n",
    "                    join_author = lambda x: clean_author(x.book_author), \n",
    "                )\n",
    "                .merge(\n",
    "                    book_df.loc[:, ['join_title', 'join_author']].drop_duplicates(), \n",
    "                    on=['join_title', 'join_author'], \n",
    "                    how='inner'\n",
    "                    )\n",
    "                .drop_duplicates(subset=['join_title', 'join_author'])\n",
    "                .drop(['book_title', 'book_author'], axis=1)\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhancing Revenue Data with TMDB\n",
    "The CMU dataset lacks comprehensive revenue data, so we supplement it with revenue information from [The Movie Database](https://www.themoviedb.org)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading the-movies-dataset.zip to /Users/nifleisch/dev/uni/ada-2023\n",
      "  0%|                                                | 0.00/228M [00:00<?, ?B/s]^C\n",
      "  0%|                                                | 0.00/228M [00:00<?, ?B/s]\n",
      "User cancelled operation\n",
      "Archive:  the-movies-dataset.zip\n",
      "  End-of-central-directory signature not found.  Either this file is not\n",
      "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
      "  latter case the central directory and zipfile comment will be found on\n",
      "  the last disk(s) of this archive.\n",
      "unzip:  cannot find zipfile directory in one of the-movies-dataset.zip or\n",
      "        the-movies-dataset.zip.zip, and cannot find the-movies-dataset.zip.ZIP, period.\n",
      "mv: movies_metadata.csv: No such file or directory\n",
      "rm: credits.csv: No such file or directory\n",
      "rm: keywords.csv: No such file or directory\n",
      "rm: links.csv: No such file or directory\n",
      "rm: links_small.csv: No such file or directory\n",
      "rm: ratings.csv: No such file or directory\n",
      "rm: ratings_small.csv: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets download -d rounakbanik/the-movies-dataset\n",
    "!unzip the-movies-dataset.zip\n",
    "!rm the-movies-dataset.zip\n",
    "!mv movies_metadata.csv data/\n",
    "!rm credits.csv\n",
    "!rm keywords.csv\n",
    "!rm links.csv\n",
    "!rm links_small.csv\n",
    "!rm ratings.csv\n",
    "!rm ratings_small.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/73/4f40pvqd15d7bgql2yrmyvzm0000gn/T/ipykernel_18097/2615940317.py:5: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  tmdb_df = (pd.read_csv(\"data/movies_metadata.csv\")\n"
     ]
    }
   ],
   "source": [
    "def replace_jpg(x):\n",
    "    return np.nan if isinstance(x, str) and x.endswith('.jpg') else x\n",
    "\n",
    "\n",
    "tmdb_df = (pd.read_csv(\"data/movies_metadata.csv\")\n",
    "            .assign(\n",
    "                    movie_budget = lambda df: df.budget.apply(replace_jpg).astype(\"Int64\").replace(0, pd.NA),\n",
    "                    movie_revenue_tmdb = lambda df: df.revenue.replace(0.0, pd.NA).astype(\"Int64\")\n",
    "            )\n",
    "            .loc[:, ['imdb_id', 'movie_budget', 'movie_revenue_tmdb']]\n",
    "            .drop_duplicates(subset=['imdb_id'])\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing Revenues and Budgets with CPI\n",
    "To make 70 years' worth of revenue and budget data comparable, we adjust for inflation using the US Consumer Price Index (CPI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "package = Package('https://datahub.io/core/cpi-us/datapackage.json')\n",
    "cpi_df = (pd.DataFrame(package.resources[1].read(), columns=['date', 'cpi', '_'])\n",
    "          .assign(\n",
    "                year = lambda df: df.date.astype(str).str.slice(0, 4).astype(\"Int32\"),\n",
    "                inflation_adjustment = lambda df: (df.cpi.iloc[-1] / df.cpi).astype(float)\n",
    "            )\n",
    "          .drop(columns=['date', 'cpi', '_'])\n",
    "          .drop_duplicates(subset=['year'])\n",
    "          .reset_index(drop=True)\n",
    "          .assign(inflation_adjustment = lambda x: x.inflation_adjustment.astype(float))\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDb Ratings\n",
    "We load IMDb movie ratings to assess the 'goodness' of movies according to users' opinions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 6758k  100 6758k    0     0  9754k      0 --:--:-- --:--:-- --:--:-- 9852k\n",
      "rm: title.ratings.tsv.gz: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!curl -o title.ratings.tsv.gz https://datasets.imdbws.com/title.ratings.tsv.gz\n",
    "!gunzip title.ratings.tsv.gz\n",
    "!mv title.ratings.tsv data/\n",
    "!rm title.ratings.tsv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_df = (pd.read_csv(\"data/title.ratings.tsv\", sep='\\t')\n",
    "            .rename(columns={\n",
    "                'tconst': 'imdb_id', \n",
    "                'averageRating': 'imdb_rating', \n",
    "                'numVotes': 'imdb_total_votes'})\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB ID\n",
    "To join IMDb and TMDB datasets, we query IMDb IDs from Wikidata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT ?movie ?IMDB_ID\n",
    "WHERE\n",
    "{\n",
    "VALUES ?movieType { wd:Q11424 wd:Q506240 }\n",
    "?movie wdt:P31 ?movieType.\n",
    "?movie wdt:P345 ?IMDB_ID.\n",
    "\n",
    "SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "}\n",
    "'''\n",
    "query_result = requests.get(WIKI_DATA_SERVICE_URL, params = {'format': 'json', 'query': query})\n",
    "wikidata_imdb_df =pd.DataFrame(query_result.json()['results']['bindings'])\n",
    "for column in wikidata_imdb_df.columns:\n",
    "    wikidata_imdb_df[column] = wikidata_imdb_df[column].apply(lambda x: x['value'] if isinstance(x, dict) and 'value' in x else x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikidata_imdb_df = (wikidata_imdb_df\n",
    "                    .assign(\n",
    "                        movie_wikidata_id = lambda x: x.movie.str.split('/').str[-1],\n",
    "                        imdb_id = lambda x: x.IMDB_ID\n",
    "                    )\n",
    "                    .loc[:, ['movie_wikidata_id', 'imdb_id']]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_adaptation_df = (cmu_df\n",
    "                        .merge(wikidata_imdb_df, on='movie_wikidata_id', how='left')\n",
    "                        .merge(imdb_df, on='imdb_id', how='left')\n",
    "                        .merge(tmdb_df, on='imdb_id', how='left')\n",
    "                        .merge(wikidata_df, on='movie_wikidata_id', how='left')\n",
    "                        .merge(goodreads_df, on=['join_title', 'join_author'], how='left')\n",
    "                        .merge(cpi_df, left_on='movie_release', right_on='year', how='left')\n",
    "                        .assign(\n",
    "                            movie_budget = lambda x: x.movie_budget.astype(float),\n",
    "                            movie_revenue = lambda x: x.movie_revenue.fillna(x.movie_revenue_tmdb).astype(float),\n",
    "                            movie_is_adaptation = lambda x: x.book_wikidata_id.notna()\n",
    "                            )\n",
    "                        .assign(\n",
    "                            movie_budget = lambda df: df.movie_budget * df.inflation_adjustment,\n",
    "                            movie_revenue = lambda df: df.movie_revenue * df.inflation_adjustment\n",
    "                        )\n",
    "                        .drop(columns=['movie_wikidata_id', 'imdb_id', 'movie_revenue_tmdb', 'book_goodreads_id',\n",
    "                                        'year', 'inflation_adjustment', 'join_title', 'join_author'])\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_adaptation_df.to_csv(\"book_adaptation.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "(book_df\n",
    " .merge(goodreads_book_df, on=['join_title', 'join_author'], how='inner')\n",
    " .drop(columns=['join_title', 'join_author'])\n",
    " .to_csv(\"book.csv\", index=False)\n",
    " )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
